{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f29863-a842-4c29-917c-f3fd5680e4da",
   "metadata": {},
   "source": [
    "# Data collation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485e53c3-2a6a-46f1-b7d7-99f9359bc740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b452f6-c48e-4c05-9805-24b683142d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base project path\n",
    "project_root = Path(os.getcwd()).resolve().parent  # change as needed\n",
    "interim_base = project_root / \"data\" / \"interim\"\n",
    "\n",
    "da_folder = interim_base / \"da\"\n",
    "rt_folder = interim_base / \"rt\"\n",
    "\n",
    "assert da_folder.exists() or rt_folder.exists(), \"Make sure the interim/da or interim/rt folders exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa587e4-4b5f-4c06-909a-d9caf2503f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 DA files and 5 RT files.\n"
     ]
    }
   ],
   "source": [
    "# find files\n",
    "da_files = sorted(glob.glob(str(da_folder / \"data_da_*_cleaned_no_neg.csv\")))\n",
    "rt_files = sorted(glob.glob(str(rt_folder / \"data_rt_*_cleaned_no_neg.csv\")))\n",
    "\n",
    "print(f\"Found {len(da_files)} DA files and {len(rt_files)} RT files.\")\n",
    "\n",
    "# helper: canonical asset id column name (common names)\n",
    "def _asset_id_col(df):\n",
    "    for candidate in [\"Masked Asset ID\", \"asset_id\", \"asset id\", \"Asset ID\"]:\n",
    "        if candidate in df.columns:\n",
    "            return candidate\n",
    "    # fallback: first numeric-like column with many unique values\n",
    "    return df.columns[0]\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def read_and_prepare(path, market_type):\n",
    "    \"\"\"\n",
    "    Read CSV and return df with:\n",
    "      - canonical timestamp '_ts' (prefers int_start / interval_start),\n",
    "      - 'date' (YYYY-MM-DD str) and 'hour' (HH:MM:SS) derived from _ts,\n",
    "      - market_type and source_file columns.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    df = pd.read_csv(path)\n",
    "    src = path.name\n",
    "\n",
    "    # normalize column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # rename asset id\n",
    "    aid = _asset_id_col(df)\n",
    "    df = df.rename(columns={aid: \"asset_id\"})\n",
    "\n",
    "    # map lower->original for quick lookup\n",
    "    cols_low = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    # priority: int_start / interval_start, then int_end / interval_end, then ts/timestamp, then Date+hour\n",
    "    start_candidates = [\"int_start\", \"interval_start\", \"intervalstart\", \"intstart\"]\n",
    "    end_candidates   = [\"int_end\", \"interval_end\", \"intervalend\", \"intend\"]\n",
    "    ts_candidates    = [\"ts\", \"timestamp\", \"time_stamp\"]\n",
    "\n",
    "    def find_col(cands):\n",
    "        for x in cands:\n",
    "            if x in cols_low:\n",
    "                return cols_low[x]\n",
    "        return None\n",
    "\n",
    "    ts_col = find_col(start_candidates) or find_col(end_candidates) or find_col(ts_candidates)\n",
    "\n",
    "    if ts_col:\n",
    "        df[\"_ts\"] = pd.to_datetime(df[ts_col], errors=\"coerce\")\n",
    "    else:\n",
    "        # fallback: Date + hour\n",
    "        date_col = next((cols_low[c] for c in (\"date\", \"day\") if c in cols_low), None)\n",
    "        hour_col = next((cols_low[c] for c in (\"hour\", \"time\", \"hour_of_day\") if c in cols_low), None)\n",
    "        if date_col and hour_col:\n",
    "            df[\"_ts\"] = pd.to_datetime(df[date_col].astype(str) + \" \" + df[hour_col].astype(str),\n",
    "                                       errors=\"coerce\")\n",
    "        elif date_col:\n",
    "            df[\"_ts\"] = pd.to_datetime(df[date_col].astype(str), errors=\"coerce\")\n",
    "        else:\n",
    "            # last resort: try parsing any column that looks like datetimes\n",
    "            parsed = None\n",
    "            for c in df.columns:\n",
    "                if df[c].dtype == object:\n",
    "                    cand = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "                    if not cand.isna().all():\n",
    "                        parsed = cand\n",
    "                        break\n",
    "            df[\"_ts\"] = parsed if parsed is not None else pd.NaT\n",
    "\n",
    "    # If some rows still NaT, try to fill from any column that contains 'int' + 'start'/'end' in the name\n",
    "    if df[\"_ts\"].isna().any():\n",
    "        for c in df.columns:\n",
    "            cl = c.lower()\n",
    "            if (\"int\" in cl and (\"start\" in cl or \"end\" in cl)) or (\"interval\" in cl and (\"start\" in cl or \"end\" in cl)):\n",
    "                candidate = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "                fill_mask = df[\"_ts\"].isna() & ~candidate.isna()\n",
    "                if fill_mask.any():\n",
    "                    df.loc[fill_mask, \"_ts\"] = candidate[fill_mask]\n",
    "\n",
    "    # Build date and hour from _ts (hour reflects the interval start time)\n",
    "    df[\"date\"] = df[\"_ts\"].dt.date.astype(\"str\")\n",
    "    df[\"hour\"] = df[\"_ts\"].dt.strftime(\"%H:%M:%S\")    # HH:MM:SS from interval_start\n",
    "\n",
    "    df[\"market_type\"] = market_type\n",
    "    df[\"source_file\"] = src\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e864dc2-f9d4-4c4e-988f-6c96ca657e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved wide consolidated -> C:\\Users\\Mitra's\\ISO-Ne-Devasya\\data\\processed\\master_data_wide.csv (shape: (10044, 41))\n"
     ]
    }
   ],
   "source": [
    "# read\n",
    "da_dfs = [read_and_prepare(p, \"da\") for p in da_files]\n",
    "rt_dfs = [read_and_prepare(p, \"rt\") for p in rt_files]\n",
    "\n",
    "# sort within each dataframe by timestamp and asset id\n",
    "def _sort_df(df):\n",
    "    if \"_ts\" not in df.columns:\n",
    "        df[\"_ts\"] = pd.NaT\n",
    "    return df.sort_values([\"_ts\", \"asset_id\"]).reset_index(drop=True)\n",
    "\n",
    "da_dfs = [_sort_df(df) for df in da_dfs]\n",
    "rt_dfs = [_sort_df(df) for df in rt_dfs]\n",
    "\n",
    "# concat DA first then RT\n",
    "wide_da = pd.concat(da_dfs, ignore_index=True) if da_dfs else pd.DataFrame()\n",
    "wide_rt = pd.concat(rt_dfs, ignore_index=True) if rt_dfs else pd.DataFrame()\n",
    "wide_all = pd.concat([wide_da, wide_rt], ignore_index=True)\n",
    "\n",
    "# ensure final chronological order inside each market and DA above RT\n",
    "wide_all[\"_market_order\"] = wide_all[\"market_type\"].map({\"da\": 0, \"rt\": 1}).fillna(1)\n",
    "wide_all = wide_all.sort_values([\"_market_order\", \"_ts\", \"asset_id\"]).reset_index(drop=True)\n",
    "wide_all = wide_all.drop(columns=[\"_market_order\"])\n",
    "\n",
    "# save wide to processed folder\n",
    "processed_folder = project_root / \"data\" / \"processed\"\n",
    "processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "wide_path = processed_folder / \"master_data_wide.csv\"\n",
    "wide_all.to_csv(wide_path, index=False)\n",
    "print(f\"Saved wide consolidated -> {wide_path} (shape: {wide_all.shape})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e568889a-3588-438a-b6d4-cb2d2157f384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved wide consolidated to: C:\\Users\\Mitra's\\ISO-Ne-Devasya\\data\\processed\\master_data_wide.csv\n",
      "Wide shape: (10044, 41)\n"
     ]
    }
   ],
   "source": [
    "# concat DA then RT (they are each internally sorted)\n",
    "wide_da = pd.concat(da_dfs, ignore_index=True) if da_dfs else pd.DataFrame()\n",
    "wide_rt = pd.concat(rt_dfs, ignore_index=True) if rt_dfs else pd.DataFrame()\n",
    "\n",
    "# final wide dataset: DA first then RT\n",
    "wide_all = pd.concat([wide_da, wide_rt], ignore_index=True)\n",
    "\n",
    "# optional: ensure a clear chronological index column for the whole DF\n",
    "wide_all = wide_all.sort_values([\"market_type\", \"_ts\", \"asset_id\"]).reset_index(drop=True)\n",
    "\n",
    "# save to processed folder\n",
    "processed_folder = project_root / \"data\" / \"processed\"\n",
    "processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "wide_path = processed_folder / \"master_data_wide.csv\"\n",
    "wide_all.to_csv(wide_path, index=False)\n",
    "\n",
    "print(f\"Saved wide consolidated to: {wide_path}\")\n",
    "print(\"Wide shape:\", wide_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e9969b-3250-4c81-98ff-e7e17cea48c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wide_all.shape: (10044, 41)\n",
      "\n",
      "Columns (first 50):\n",
      "['Date',\n",
      " 'trade_int',\n",
      " 'lead_id',\n",
      " 'asset_id',\n",
      " 'must_take_eng',\n",
      " 'max_eng',\n",
      " 'eco_max',\n",
      " 'eco_min',\n",
      " 'cold_price',\n",
      " 'inter_price',\n",
      " 'hot_price',\n",
      " 'no_load_price',\n",
      " 'seg1_price',\n",
      " 'seg1_mw',\n",
      " 'seg2_price',\n",
      " 'seg2_mw',\n",
      " 'seg3_price',\n",
      " 'seg3_mw',\n",
      " 'seg4_price',\n",
      " 'seg4_mw',\n",
      " 'seg5_price',\n",
      " 'seg5_mw',\n",
      " 'seg6_price',\n",
      " 'seg6_mw',\n",
      " 'seg7_price',\n",
      " 'seg7_mw',\n",
      " 'seg8_price',\n",
      " 'seg8_mw',\n",
      " 'seg9_price',\n",
      " 'seg9_mw',\n",
      " 'seg10_price',\n",
      " 'seg10_mw',\n",
      " 'claim10',\n",
      " 'claim30',\n",
      " 'int_start',\n",
      " 'int_end',\n",
      " '_ts',\n",
      " 'date',\n",
      " 'hour',\n",
      " 'market_type',\n",
      " 'source_file']\n",
      "\n",
      "First 6 rows preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>trade_int</th>\n",
       "      <th>lead_id</th>\n",
       "      <th>asset_id</th>\n",
       "      <th>must_take_eng</th>\n",
       "      <th>max_eng</th>\n",
       "      <th>eco_max</th>\n",
       "      <th>eco_min</th>\n",
       "      <th>cold_price</th>\n",
       "      <th>inter_price</th>\n",
       "      <th>...</th>\n",
       "      <th>seg10_mw</th>\n",
       "      <th>claim10</th>\n",
       "      <th>claim30</th>\n",
       "      <th>int_start</th>\n",
       "      <th>int_end</th>\n",
       "      <th>_ts</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>market_type</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>101197</td>\n",
       "      <td>11031</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1226.23</td>\n",
       "      <td>1226.23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-06-25 00:00:00</td>\n",
       "      <td>2025-06-25 00:59:59.999</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>591975</td>\n",
       "      <td>12297</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>11809.83</td>\n",
       "      <td>8385.43</td>\n",
       "      <td>...</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-06-25 00:00:00</td>\n",
       "      <td>2025-06-25 00:59:59.999</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>872788</td>\n",
       "      <td>17063</td>\n",
       "      <td>0</td>\n",
       "      <td>8880.0</td>\n",
       "      <td>326.3</td>\n",
       "      <td>179.5</td>\n",
       "      <td>21921.03</td>\n",
       "      <td>10542.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-06-25 00:00:00</td>\n",
       "      <td>2025-06-25 00:59:59.999</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>412080</td>\n",
       "      <td>17698</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3336.74</td>\n",
       "      <td>3336.74</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>2025-06-25 00:00:00</td>\n",
       "      <td>2025-06-25 00:59:59.999</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>101197</td>\n",
       "      <td>18898</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1944.97</td>\n",
       "      <td>1944.97</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-06-25 00:00:00</td>\n",
       "      <td>2025-06-25 00:59:59.999</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>101197</td>\n",
       "      <td>19426</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1226.23</td>\n",
       "      <td>1226.23</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-06-25 00:00:00</td>\n",
       "      <td>2025-06-25 00:59:59.999</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  trade_int  lead_id  asset_id  must_take_eng  max_eng  eco_max  \\\n",
       "0  2025-06-25          1   101197     11031              0      0.0     44.0   \n",
       "1  2025-06-25          1   591975     12297              0      0.0    266.0   \n",
       "2  2025-06-25          1   872788     17063              0   8880.0    326.3   \n",
       "3  2025-06-25          1   412080     17698              0      0.0     36.0   \n",
       "4  2025-06-25          1   101197     18898              0      0.0     46.0   \n",
       "5  2025-06-25          1   101197     19426              0      0.0     42.0   \n",
       "\n",
       "   eco_min  cold_price  inter_price  ...  seg10_mw  claim10  claim30  \\\n",
       "0     38.0     1226.23      1226.23  ...       NaN      0.0      0.0   \n",
       "1    155.0    11809.83      8385.43  ...      7.00      0.0      0.0   \n",
       "2    179.5    21921.03     10542.31  ...      0.05      0.0      0.0   \n",
       "3     36.0     3336.74      3336.74  ...       NaN      0.0     30.1   \n",
       "4     38.0     1944.97      1944.97  ...       NaN      0.0      0.0   \n",
       "5     38.0     1226.23      1226.23  ...       NaN      0.0      0.0   \n",
       "\n",
       "             int_start                  int_end        _ts        date  \\\n",
       "0  2025-06-25 00:00:00  2025-06-25 00:59:59.999 2025-06-25  2025-06-25   \n",
       "1  2025-06-25 00:00:00  2025-06-25 00:59:59.999 2025-06-25  2025-06-25   \n",
       "2  2025-06-25 00:00:00  2025-06-25 00:59:59.999 2025-06-25  2025-06-25   \n",
       "3  2025-06-25 00:00:00  2025-06-25 00:59:59.999 2025-06-25  2025-06-25   \n",
       "4  2025-06-25 00:00:00  2025-06-25 00:59:59.999 2025-06-25  2025-06-25   \n",
       "5  2025-06-25 00:00:00  2025-06-25 00:59:59.999 2025-06-25  2025-06-25   \n",
       "\n",
       "       hour  market_type                          source_file  \n",
       "0  00:00:00           da  data_da_20250625_cleaned_no_neg.csv  \n",
       "1  00:00:00           da  data_da_20250625_cleaned_no_neg.csv  \n",
       "2  00:00:00           da  data_da_20250625_cleaned_no_neg.csv  \n",
       "3  00:00:00           da  data_da_20250625_cleaned_no_neg.csv  \n",
       "4  00:00:00           da  data_da_20250625_cleaned_no_neg.csv  \n",
       "5  00:00:00           da  data_da_20250625_cleaned_no_neg.csv  \n",
       "\n",
       "[6 rows x 41 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected seg_mw columns:\n",
      "['seg1_mw',\n",
      " 'seg2_mw',\n",
      " 'seg3_mw',\n",
      " 'seg4_mw',\n",
      " 'seg5_mw',\n",
      " 'seg6_mw',\n",
      " 'seg7_mw',\n",
      " 'seg8_mw',\n",
      " 'seg9_mw',\n",
      " 'seg10_mw']\n",
      "\n",
      "Detected seg_price-like columns:\n",
      "['seg1_price',\n",
      " 'seg2_price',\n",
      " 'seg3_price',\n",
      " 'seg4_price',\n",
      " 'seg5_price',\n",
      " 'seg6_price',\n",
      " 'seg7_price',\n",
      " 'seg8_price',\n",
      " 'seg9_price',\n",
      " 'seg10_price']\n",
      "\n",
      "seg MW columns diagnostics:\n",
      " - seg1_mw | dtype=float64 | non-null=10044 | sample values=['43.0', '155.0', '179.0', '35.0', '41.0', '184.0']\n",
      " - seg2_mw | dtype=float64 | non-null=10044 | sample values=['1.0', '69.0', '3.0', '7.0', '40.0', '2.9']\n",
      " - seg3_mw | dtype=float64 | non-null=6442 | sample values=['14.0', '68.0', '7.0', '15.0', '140.0', '10.0']\n",
      " - seg4_mw | dtype=float64 | non-null=6442 | sample values=['14.0', '10.0', '7.0', '15.0', '60.0', '17.0']\n",
      " - seg5_mw | dtype=float64 | non-null=6442 | sample values=['14.0', '0.0500000000000019', '7.0', '15.0', '50.0', '10.0']\n",
      " - seg6_mw | dtype=float64 | non-null=6442 | sample values=['40.0', '0.0500000000000019', '7.0', '45.0', '50.0', '26.0']\n",
      " - seg7_mw | dtype=float64 | non-null=6442 | sample values=['1.0', '0.0500000000000019', '8.0', '50.0', '7.0', '16.0']\n",
      " - seg8_mw | dtype=float64 | non-null=6442 | sample values=['5.0', '0.0500000000000019', '7.0', '50.0', '6.0', '1.0']\n",
      " - seg9_mw | dtype=float64 | non-null=6442 | sample values=['15.0', '0.0500000000000019', '8.0', '85.0', '14.0', '19.0']\n",
      " - seg10_mw | dtype=float64 | non-null=6442 | sample values=['7.0', '0.0500000000000019', '26.0', '1.0', '15.0', '11.0']\n",
      "\n",
      "_ts present: True | NaT count: 0\n",
      "Sample _ts values: <DatetimeArray>\n",
      "['2025-06-25 00:00:00', '2025-06-25 01:00:00', '2025-06-25 02:00:00',\n",
      " '2025-06-25 03:00:00', '2025-06-25 04:00:00', '2025-06-25 05:00:00']\n",
      "Length: 6, dtype: datetime64[ns]\n",
      "\n",
      "market_type unique values: ['da' 'rt']\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic cell for wide_all and segX_mw columns\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"wide_all.shape:\", wide_all.shape)\n",
    "print(\"\\nColumns (first 50):\")\n",
    "pprint(list(wide_all.columns)[:50])\n",
    "\n",
    "print(\"\\nFirst 6 rows preview:\")\n",
    "display(wide_all.head(6))\n",
    "\n",
    "# find segX_mw and segX_price columns explicitly (1..10)\n",
    "seg_mw_cols = []\n",
    "seg_price_cols = []\n",
    "for i in range(1, 11):\n",
    "    # possible variants\n",
    "    for cand in (f\"seg{i}_mw\", f\"seg_{i}_mw\", f\"segment {i} mw\", f\"Segment {i} MW\", f\"Segment_{i}_MW\", f\"seg{i}mw\"):\n",
    "        matches = [c for c in wide_all.columns if c.lower() == cand.lower()]\n",
    "        if matches:\n",
    "            seg_mw_cols.append(matches[0])\n",
    "            break\n",
    "    # price\n",
    "    for cand in (f\"seg{i}_price\", f\"seg_{i}_price\", f\"seg{i}_prc\", f\"segment {i} price\"):\n",
    "        matches = [c for c in wide_all.columns if c.lower() == cand.lower()]\n",
    "        if matches:\n",
    "            seg_price_cols.append(matches[0])\n",
    "            break\n",
    "\n",
    "# fallback: regex detect any col with digit + mw/prc/price anywhere\n",
    "if not seg_mw_cols:\n",
    "    seg_mw_cols = sorted([c for c in wide_all.columns if re.search(r'\\bseg[_\\s\\-]?\\d+[_\\s\\-]?mw\\b', c, re.I)])\n",
    "if not seg_price_cols:\n",
    "    seg_price_cols = sorted([c for c in wide_all.columns if re.search(r'(price|prc)', c, re.I) and re.search(r'\\d', c)])\n",
    "\n",
    "print(\"\\nDetected seg_mw columns:\")\n",
    "pprint(seg_mw_cols)\n",
    "print(\"\\nDetected seg_price-like columns:\")\n",
    "pprint(seg_price_cols)\n",
    "\n",
    "# show dtype and non-null counts and some sample non-null values for each seg mw col\n",
    "print(\"\\nseg MW columns diagnostics:\")\n",
    "for c in seg_mw_cols:\n",
    "    n_non = int(wide_all[c].notna().sum())\n",
    "    dtype = wide_all[c].dtype\n",
    "    # sample up to 6 non-null unique values\n",
    "    sample_vals = wide_all[c].dropna().astype(str).unique()[:6].tolist()\n",
    "    print(f\" - {c} | dtype={dtype} | non-null={n_non} | sample values={sample_vals}\")\n",
    "\n",
    "# show if _ts is present and some examples\n",
    "if \"_ts\" in wide_all.columns:\n",
    "    n_nat = int(wide_all[\"_ts\"].isna().sum())\n",
    "    print(f\"\\n_ts present: True | NaT count: {n_nat}\")\n",
    "    print(\"Sample _ts values:\", wide_all[\"_ts\"].dropna().unique()[:6])\n",
    "else:\n",
    "    print(\"\\n_ts column not present in wide_all\")\n",
    "\n",
    "# confirm market types present\n",
    "if \"market_type\" in wide_all.columns:\n",
    "    print(\"\\nmarket_type unique values:\", wide_all[\"market_type\"].unique())\n",
    "else:\n",
    "    print(\"\\nmarket_type column not present\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88aab959-3efc-4b86-b5ae-a538ba443771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen MW columns per segment (by max non-null):\n",
      "  Segment 1: seg1_mw | non-null = 10044\n",
      "  Segment 2: seg2_mw | non-null = 10044\n",
      "  Segment 3: seg3_mw | non-null = 6442\n",
      "  Segment 4: seg4_mw | non-null = 6442\n",
      "  Segment 5: seg5_mw | non-null = 6442\n",
      "  Segment 6: seg6_mw | non-null = 6442\n",
      "  Segment 7: seg7_mw | non-null = 6442\n",
      "  Segment 8: seg8_mw | non-null = 6442\n",
      "  Segment 9: seg9_mw | non-null = 6442\n",
      "  Segment 10: seg10_mw | non-null = 6442\n",
      "\n",
      "Chosen Price columns per segment (by max non-null):\n",
      "  Segment 1: seg1_price | non-null = 10044\n",
      "  Segment 2: seg2_price | non-null = 10044\n",
      "  Segment 3: seg3_price | non-null = 6442\n",
      "  Segment 4: seg4_price | non-null = 6442\n",
      "  Segment 5: seg5_price | non-null = 6442\n",
      "  Segment 6: seg6_price | non-null = 6442\n",
      "  Segment 7: seg7_price | non-null = 6442\n",
      "  Segment 8: seg8_price | non-null = 6442\n",
      "  Segment 9: seg9_price | non-null = 6442\n",
      "  Segment 10: seg10_price | non-null = 6442\n",
      "\n",
      "After coercion: non-null counts by chosen MW column\n",
      "  Segment 1: seg1_mw non-null = 10044\n",
      "  Segment 2: seg2_mw non-null = 10044\n",
      "  Segment 3: seg3_mw non-null = 6442\n",
      "  Segment 4: seg4_mw non-null = 6442\n",
      "  Segment 5: seg5_mw non-null = 6442\n",
      "  Segment 6: seg6_mw non-null = 6442\n",
      "  Segment 7: seg7_mw non-null = 6442\n",
      "  Segment 8: seg8_mw non-null = 6442\n",
      "  Segment 9: seg9_mw non-null = 6442\n",
      "  Segment 10: seg10_mw non-null = 6442\n",
      " Segment 1: kept rows = 10044 (from column: seg1_mw)\n",
      " Segment 2: kept rows = 10044 (from column: seg2_mw)\n",
      " Segment 3: kept rows = 6442 (from column: seg3_mw)\n",
      " Segment 4: kept rows = 6442 (from column: seg4_mw)\n",
      " Segment 5: kept rows = 6442 (from column: seg5_mw)\n",
      " Segment 6: kept rows = 6442 (from column: seg6_mw)\n",
      " Segment 7: kept rows = 6442 (from column: seg7_mw)\n",
      " Segment 8: kept rows = 6442 (from column: seg8_mw)\n",
      " Segment 9: kept rows = 6442 (from column: seg9_mw)\n",
      " Segment 10: kept rows = 6442 (from column: seg10_mw)\n",
      "\n",
      "Saved long consolidated -> C:\\Users\\Mitra's\\ISO-Ne-Devasya\\data\\processed\\master_data_long.csv  shape: (71624, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>asset_id</th>\n",
       "      <th>segment</th>\n",
       "      <th>MW</th>\n",
       "      <th>price</th>\n",
       "      <th>market_type</th>\n",
       "      <th>source_file</th>\n",
       "      <th>_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>11031</td>\n",
       "      <td>1</td>\n",
       "      <td>43.00</td>\n",
       "      <td>67.04</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>11031</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>67.04</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>1</td>\n",
       "      <td>155.00</td>\n",
       "      <td>29.40</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>51.33</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>3</td>\n",
       "      <td>14.00</td>\n",
       "      <td>52.14</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>4</td>\n",
       "      <td>14.00</td>\n",
       "      <td>53.66</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>5</td>\n",
       "      <td>14.00</td>\n",
       "      <td>55.18</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>6</td>\n",
       "      <td>40.00</td>\n",
       "      <td>58.11</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>7</td>\n",
       "      <td>1.00</td>\n",
       "      <td>64.58</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>8</td>\n",
       "      <td>5.00</td>\n",
       "      <td>78.32</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>9</td>\n",
       "      <td>15.00</td>\n",
       "      <td>108.79</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>12297</td>\n",
       "      <td>10</td>\n",
       "      <td>7.00</td>\n",
       "      <td>277.61</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>1</td>\n",
       "      <td>179.00</td>\n",
       "      <td>14.95</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>2</td>\n",
       "      <td>69.00</td>\n",
       "      <td>55.54</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>3</td>\n",
       "      <td>68.00</td>\n",
       "      <td>69.89</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>4</td>\n",
       "      <td>10.00</td>\n",
       "      <td>69.90</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>97.37</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>6</td>\n",
       "      <td>0.05</td>\n",
       "      <td>115.29</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>7</td>\n",
       "      <td>0.05</td>\n",
       "      <td>133.21</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>151.13</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>9</td>\n",
       "      <td>0.05</td>\n",
       "      <td>169.05</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17063</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>186.97</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17698</td>\n",
       "      <td>1</td>\n",
       "      <td>35.00</td>\n",
       "      <td>180.93</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>17698</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>180.93</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>18898</td>\n",
       "      <td>1</td>\n",
       "      <td>43.00</td>\n",
       "      <td>100.75</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>18898</td>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>100.75</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>19426</td>\n",
       "      <td>1</td>\n",
       "      <td>41.00</td>\n",
       "      <td>67.04</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>19426</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>67.04</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>20846</td>\n",
       "      <td>1</td>\n",
       "      <td>184.00</td>\n",
       "      <td>22.78</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2025-06-25</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>20846</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>59.26</td>\n",
       "      <td>da</td>\n",
       "      <td>data_da_20250625_cleaned_no_neg.csv</td>\n",
       "      <td>2025-06-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date      hour  asset_id  segment      MW   price market_type  \\\n",
       "0   2025-06-25  00:00:00     11031        1   43.00   67.04          da   \n",
       "1   2025-06-25  00:00:00     11031        2    1.00   67.04          da   \n",
       "2   2025-06-25  00:00:00     12297        1  155.00   29.40          da   \n",
       "3   2025-06-25  00:00:00     12297        2    1.00   51.33          da   \n",
       "4   2025-06-25  00:00:00     12297        3   14.00   52.14          da   \n",
       "5   2025-06-25  00:00:00     12297        4   14.00   53.66          da   \n",
       "6   2025-06-25  00:00:00     12297        5   14.00   55.18          da   \n",
       "7   2025-06-25  00:00:00     12297        6   40.00   58.11          da   \n",
       "8   2025-06-25  00:00:00     12297        7    1.00   64.58          da   \n",
       "9   2025-06-25  00:00:00     12297        8    5.00   78.32          da   \n",
       "10  2025-06-25  00:00:00     12297        9   15.00  108.79          da   \n",
       "11  2025-06-25  00:00:00     12297       10    7.00  277.61          da   \n",
       "12  2025-06-25  00:00:00     17063        1  179.00   14.95          da   \n",
       "13  2025-06-25  00:00:00     17063        2   69.00   55.54          da   \n",
       "14  2025-06-25  00:00:00     17063        3   68.00   69.89          da   \n",
       "15  2025-06-25  00:00:00     17063        4   10.00   69.90          da   \n",
       "16  2025-06-25  00:00:00     17063        5    0.05   97.37          da   \n",
       "17  2025-06-25  00:00:00     17063        6    0.05  115.29          da   \n",
       "18  2025-06-25  00:00:00     17063        7    0.05  133.21          da   \n",
       "19  2025-06-25  00:00:00     17063        8    0.05  151.13          da   \n",
       "20  2025-06-25  00:00:00     17063        9    0.05  169.05          da   \n",
       "21  2025-06-25  00:00:00     17063       10    0.05  186.97          da   \n",
       "22  2025-06-25  00:00:00     17698        1   35.00  180.93          da   \n",
       "23  2025-06-25  00:00:00     17698        2    1.00  180.93          da   \n",
       "24  2025-06-25  00:00:00     18898        1   43.00  100.75          da   \n",
       "25  2025-06-25  00:00:00     18898        2    3.00  100.75          da   \n",
       "26  2025-06-25  00:00:00     19426        1   41.00   67.04          da   \n",
       "27  2025-06-25  00:00:00     19426        2    1.00   67.04          da   \n",
       "28  2025-06-25  00:00:00     20846        1  184.00   22.78          da   \n",
       "29  2025-06-25  00:00:00     20846        2    1.00   59.26          da   \n",
       "\n",
       "                            source_file        _ts  \n",
       "0   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "1   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "2   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "3   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "4   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "5   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "6   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "7   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "8   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "9   data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "10  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "11  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "12  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "13  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "14  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "15  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "16  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "17  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "18  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "19  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "20  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "21  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "22  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "23  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "24  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "25  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "26  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "27  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "28  data_da_20250625_cleaned_no_neg.csv 2025-06-25  \n",
       "29  data_da_20250625_cleaned_no_neg.csv 2025-06-25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = list(wide_all.columns)\n",
    "seg_mw_map = {}\n",
    "seg_price_map = {}\n",
    "\n",
    "def candidate_cols_for_seg(n):\n",
    "    \"\"\"Return columns that likely correspond to segment n and MW or price.\"\"\"\n",
    "    nstr = str(n)\n",
    "    mw_cands = []\n",
    "    price_cands = []\n",
    "    for c in cols:\n",
    "        lc = c.lower()\n",
    "        # candidate if it contains the segment number and 'mw'\n",
    "        if re.search(r'\\b' + re.escape(nstr) + r'\\b', lc) and 'mw' in lc:\n",
    "            mw_cands.append(c)\n",
    "        # allow seg1 in condensed names like 'seg1_mw' (no word boundary)\n",
    "        if re.search(r'seg[_\\-\\s]*' + re.escape(nstr), lc) and 'mw' in lc:\n",
    "            mw_cands.append(c)\n",
    "        # price candidates: number + price/prc or segN_price etc\n",
    "        if (re.search(r'\\b' + re.escape(nstr) + r'\\b', lc) or re.search(r'seg[_\\-\\s]*' + re.escape(nstr), lc)) \\\n",
    "           and ('price' in lc or 'prc' in lc):\n",
    "            price_cands.append(c)\n",
    "    # also include any 'Segment N MW' style (space) matches\n",
    "    for c in cols:\n",
    "        lc = c.lower()\n",
    "        if f\"segment {n}\" in lc and 'mw' in lc:\n",
    "            if c not in mw_cands:\n",
    "                mw_cands.append(c)\n",
    "        if f\"segment {n}\" in lc and ('price' in lc or 'prc' in lc):\n",
    "            if c not in price_cands:\n",
    "                price_cands.append(c)\n",
    "    # deduplicate\n",
    "    mw_cands = list(dict.fromkeys(mw_cands))\n",
    "    price_cands = list(dict.fromkeys(price_cands))\n",
    "    return mw_cands, price_cands\n",
    "\n",
    "# pick best column (highest non-null count) among candidates\n",
    "for n in range(1, 11):\n",
    "    mw_cands, price_cands = candidate_cols_for_seg(n)\n",
    "    best_mw = None\n",
    "    best_price = None\n",
    "\n",
    "    if mw_cands:\n",
    "        counts = {c: int(wide_all[c].notna().sum()) for c in mw_cands}\n",
    "        # choose column with max non-null\n",
    "        best_mw = max(counts, key=lambda k: counts[k])\n",
    "        seg_mw_map[n] = best_mw\n",
    "\n",
    "    if price_cands:\n",
    "        counts_p = {c: int(wide_all[c].notna().sum()) for c in price_cands}\n",
    "        best_price = max(counts_p, key=lambda k: counts_p[k])\n",
    "        seg_price_map[n] = best_price\n",
    "\n",
    "print(\"Chosen MW columns per segment (by max non-null):\")\n",
    "for n in sorted(seg_mw_map.keys()):\n",
    "    c = seg_mw_map[n]\n",
    "    print(f\"  Segment {n}: {c} | non-null = {int(wide_all[c].notna().sum())}\")\n",
    "\n",
    "print(\"\\nChosen Price columns per segment (by max non-null):\")\n",
    "for n in sorted(seg_price_map.keys()):\n",
    "    c = seg_price_map[n]\n",
    "    print(f\"  Segment {n}: {c} | non-null = {int(wide_all[c].notna().sum())}\")\n",
    "\n",
    "if not seg_mw_map:\n",
    "    raise RuntimeError(\"No MW candidate columns found. Please run diagnostic and paste column list here.\")\n",
    "\n",
    "# Coerce chosen columns to numeric (remove commas/spaces)\n",
    "for n, col in seg_mw_map.items():\n",
    "    wide_all[col] = pd.to_numeric(wide_all[col].astype(str).str.replace(\",\", \"\").str.strip(), errors=\"coerce\")\n",
    "for n, col in seg_price_map.items():\n",
    "    wide_all[col] = pd.to_numeric(wide_all[col].astype(str).str.replace(\",\", \"\").str.strip(), errors=\"coerce\")\n",
    "\n",
    "# Report counts after coercion\n",
    "print(\"\\nAfter coercion: non-null counts by chosen MW column\")\n",
    "for n in sorted(seg_mw_map.keys()):\n",
    "    col = seg_mw_map[n]\n",
    "    print(f\"  Segment {n}: {col} non-null = {int(wide_all[col].notna().sum())}\")\n",
    "\n",
    "# Build long frames using chosen columns\n",
    "long_frames = []\n",
    "for seg_num in sorted(seg_mw_map.keys()):\n",
    "    mw_col = seg_mw_map[seg_num]\n",
    "    price_col = seg_price_map.get(seg_num, None)\n",
    "    tmp = pd.DataFrame({\n",
    "        \"date\": wide_all[\"_ts\"].dt.date.astype(str) if \"_ts\" in wide_all.columns else wide_all.get(\"Date\", pd.Series(index=wide_all.index, dtype=\"object\")),\n",
    "        \"hour\": wide_all[\"_ts\"].dt.strftime(\"%H:%M:%S\") if \"_ts\" in wide_all.columns else wide_all.get(\"hour\", pd.Series(index=wide_all.index, dtype=\"object\")),\n",
    "        \"asset_id\": wide_all.get(\"asset_id\", wide_all.columns[0]),\n",
    "        \"segment\": seg_num,\n",
    "        \"MW\": wide_all[mw_col],\n",
    "        \"price\": wide_all[price_col] if price_col is not None else np.nan,\n",
    "        \"market_type\": wide_all.get(\"market_type\", np.nan),\n",
    "        \"source_file\": wide_all.get(\"source_file\", np.nan),\n",
    "        \"_ts\": wide_all.get(\"_ts\", pd.NaT)\n",
    "    })\n",
    "    kept = tmp[tmp[\"MW\"].notna()].copy()\n",
    "    print(f\" Segment {seg_num}: kept rows = {len(kept)} (from column: {mw_col})\")\n",
    "    long_frames.append(kept)\n",
    "\n",
    "if not any(len(f) for f in long_frames):\n",
    "    raise RuntimeError(\"No rows retained across all segments after filtering. Please inspect outputs above.\")\n",
    "\n",
    "long_all = pd.concat(long_frames, ignore_index=True)\n",
    "long_all[\"_market_order\"] = long_all[\"market_type\"].map({\"da\": 0, \"rt\": 1}).fillna(1)\n",
    "long_all = long_all.sort_values([\"_market_order\", \"_ts\", \"asset_id\", \"segment\"]).reset_index(drop=True)\n",
    "long_all = long_all.drop(columns=[\"_market_order\"])\n",
    "\n",
    "# save long into processed folder\n",
    "processed_folder = project_root / \"data\" / \"processed\"\n",
    "processed_folder.mkdir(parents=True, exist_ok=True)\n",
    "long_path = processed_folder / \"master_data_long.csv\"\n",
    "long_all.to_csv(long_path, index=False)\n",
    "\n",
    "print(\"\\nSaved long consolidated ->\", long_path, \" shape:\", long_all.shape)\n",
    "display(long_all.head(30))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
